{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv1d.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTCB8AL-osBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koL6wrhIq_em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from time import time\n",
        "#np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "# set parameters:\n",
        "test_dim = 499\n",
        "maxlen = 100\n",
        "nb_filter = 512\n",
        "filter_length_1 = 10\n",
        "filter_length_2 = 5\n",
        "hidden_dims = 750\n",
        "nb_epoch = 20\n",
        "nb_classes = 2\n",
        "split_ratio = 0.15\n",
        "\n",
        "print('Loading data...')\n",
        "\n",
        "X = np.load('/content/drive/My Drive/Colab Notebooks/data/numpy_vectors/x_test_mfcc_500_50:50_samples_sliced_out.npy')\n",
        "y = np.load('/content/drive/My Drive/Colab Notebooks/data/numpy_vectors/y_label_500_50:50_samples_sliced_out.npy')\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9lzWC2zrkch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio)\n",
        "\n",
        "xts = X_train.shape\n",
        "#X_train = np.reshape(X_train, (xts[0], xts[1], 1))\n",
        "xtss = X_test.shape\n",
        "#X_test = np.reshape(X_test, (xtss[0], xtss[1], 1))\n",
        "yts = y_train.shape\n",
        "#y_train = np.reshape(y_train, (yts[0], 1))\n",
        "ytss = y_test.shape\n",
        "#y_test = np.reshape(y_test, (ytss[0], 1))\n",
        "\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "# print('Pad sequences (samples x time)')\n",
        "# X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "# X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "# print('X_train shape:', X_train.shape)\n",
        "# print('X_test shape:', X_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yKzDEgVroJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for batch_size in range(10, 11, 5):\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "\n",
        "    # we start off with an efficient embedding layer which maps\n",
        "    # our vocab indices into embedding_dims dimensions\n",
        "    # model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "    # model.add(Dropout(0.25))\n",
        "\n",
        "    # we add a Convolution1D, which will learn nb_filter\n",
        "    # word group filters of size filter_length:\n",
        "    model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                            filter_length=filter_length_1,\n",
        "                            input_shape=(test_dim, 13),\n",
        "                            border_mode='valid',\n",
        "                            activation='relu'\n",
        "                            ))\n",
        "    # we use standard max pooling (halving the output of the previous layer):\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                            filter_length=5,\n",
        "                            border_mode='valid',\n",
        "                            activation='relu'\n",
        "                            ))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling1D(pool_length=2))\n",
        "\n",
        "    model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                            filter_length=25,\n",
        "                            border_mode='same',\n",
        "                            activation='relu'\n",
        "                            ))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling1D(pool_length=2))\n",
        "\n",
        "    model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                            filter_length=50,\n",
        "                            border_mode='same',\n",
        "                            activation='relu'\n",
        "                            ))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling1D(pool_length=2))\n",
        "\n",
        "    model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                            filter_length=2,\n",
        "                            border_mode='same',\n",
        "                            activation='relu'\n",
        "                            ))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling1D(pool_length=2))\n",
        "\n",
        "    # We flatten the output of the conv layer,\n",
        "    # so that we can add a vanilla dense layer:\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # We add a vanilla hidden layer:\n",
        "    # model.add(Dense(hidden_dims))\n",
        "    model.add(Dropout(0.25))\n",
        "    # model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(750))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(50))\n",
        "    model.add(Activation('relu'))\n",
        "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    print(\"model/split = {} <> batchsize = {}\".format(split_ratio, batch_size))\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/split_{}_batchsize_{}\".format(split_ratio, batch_size))\n",
        "\n",
        "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "            nb_epoch=nb_epoch,  verbose=1, callbacks=[tensorboard]\t)\n",
        "\n",
        "    # model.save('model_hin_tel_38_samples.h5')\n",
        "\n",
        "    y_preds = model.predict(X_test)\n",
        "    for i in range(len(y_preds)):\n",
        "        print(y_preds[i], y_test[i])\n",
        "        \n",
        "    score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print(score)\n",
        "    print(\"\\n**********************************\\n\")\n",
        "\n",
        "# print(classification_report(Y_test, Y_preds))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}